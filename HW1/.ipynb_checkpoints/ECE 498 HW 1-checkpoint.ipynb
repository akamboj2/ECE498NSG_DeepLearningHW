{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1,\n",
       "       -1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1,  1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#truth_table:\n",
    "arr=np.array([\n",
    "[1,1,1,1,1],\n",
    "[1,1,1,1,-1],\n",
    "[1,1,1,-1,1],\n",
    "[1,1,1,-1,-1],\n",
    "[1,1,-1,1,1],\n",
    "[1,1,-1,1,-1],\n",
    "[1,1,-1,-1,1],\n",
    "[1,1,-1,-1,-1],\n",
    "[1,-1,1,1,1],\n",
    "[1,-1,1,1,-1],\n",
    "[1,-1,1,-1,1],\n",
    "[1,-1,1,-1,-1],\n",
    "[1,-1,-1,1,1],\n",
    "[1,-1,-1,1,-1],\n",
    "[1,-1,-1,-1,1],\n",
    "[1,-1,-1,-1,-1],\n",
    "[-1,1,1,1,1],\n",
    "[-1,1,1,1,-1],\n",
    "[-1,1,1,-1,1],\n",
    "[-1,1,1,-1,-1],\n",
    "[-1,1,-1,1,1],\n",
    "[-1,1,-1,1,-1],\n",
    "[-1,1,-1,-1,1],\n",
    "[-1,1,-1,-1,-1],\n",
    "[-1,-1,1,1,1],\n",
    "[-1,-1,1,1,-1],\n",
    "[-1,-1,1,-1,1],\n",
    "[-1,-1,1,-1,-1],\n",
    "[-1,-1,-1,1,1],\n",
    "[-1,-1,-1,1,-1],\n",
    "[-1,-1,-1,-1,1],\n",
    "[-1,-1,-1,-1,-1],\n",
    "]) \n",
    "\n",
    "def f(arr):\n",
    "    def xor(a,b):\n",
    "        if a==b:\n",
    "            return 1\n",
    "        elif a==-b:\n",
    "            return -1\n",
    "        \n",
    "    def or_(a,b):\n",
    "        if (a==b) and (b==1):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    v,w,x,y,z = arr\n",
    "    return or_(xor(or_(xor(v,w),x),y), xor(y,z))\n",
    "print(f([1,1,1,1,1]))\n",
    "np.apply_along_axis(f, 1, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1  1\n",
      "  1 -1 -1 -1 -1 -1 -1  1]\n",
      "Number of correct: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x is (v w x y z) data points\n",
    "# n samples\n",
    "n=100\n",
    "xs = arr\n",
    "\n",
    "xs = np.random.randint(0,2,(n,5))\n",
    "xs[ xs <1]=-1\n",
    "xs=arr\n",
    "\n",
    "y = np.apply_along_axis(f, 1, xs)\n",
    "print(y)\n",
    "w = np.zeros(5) \n",
    "b = 1\n",
    "eta = .7\n",
    "evol = []\n",
    "errors=[]\n",
    "\n",
    "for _ in range(100):\n",
    "    for i,x in enumerate(xs):\n",
    "        #print(i,x,y[i])\n",
    "        y_hat = np.sign(np.dot(x,w)+b)\n",
    "        #print(\"y_hat\",y_hat)\n",
    "        e =y[i]-y_hat\n",
    "        errors.append(e)\n",
    "        w = w + eta*e*x\n",
    "        b = b + eta*e\n",
    "        evol.append(np.append(w,b))\n",
    "    \n",
    "#print(evol)\n",
    "#print(errors)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weight Value\")\n",
    "plt.plot(range(len(evol)),evol)\n",
    "\n",
    "plt.legend([\"w1\",\"w2\",\"w3\",\"w4\",\"w5\",\"b\"])\n",
    "\n",
    "print(\"Number of correct:\", 32-np.count_nonzero(np.matmul(arr,w)-y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(arr):\n",
    "    def xor(a,b):\n",
    "        if a==b:\n",
    "            return 1\n",
    "        elif a==-b:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    def or_(a,b):\n",
    "        if (a==b) and (b==1):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    v,w,x,y,z = arr\n",
    "    return xor(xor(v,w)+x,y) + xor(y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 3\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=apply#torch.nn.Module.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\multiprocessing\\queues.py\", line 230, in _feed\n",
      "    close()\n",
      "  File \"c:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\multiprocessing\\connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"c:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\multiprocessing\\connection.py\", line 277, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [32, 32, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-570266860477>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprintnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# See note [TorchScript super()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 350\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [32, 32, 3] instead"
     ]
    }
   ],
   "source": [
    "\n",
    "net = resnet18\n",
    "input = torch.ones([32,32,3])\n",
    "# out = net(input)\n",
    "#print(out.size())\n",
    "print(resnet18)\n",
    "def printnorm(self, input, output):\n",
    "    # input is a tuple of packed inputs\n",
    "    # output is a Tensor. output.data is the Tensor we are interested\n",
    "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "    print('')\n",
    "    print('input: ', type(input))\n",
    "    print('input[0]: ', type(input[0]))\n",
    "    print('output: ', type(output))\n",
    "    print('')\n",
    "    print('input size:', input[0].size())\n",
    "    print('output size:', output.data.size())\n",
    "    print('output norm:', output.data.norm())\n",
    "\n",
    "\n",
    "net.register_forward_hook(printnorm)\n",
    "\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.  , 1560.25,    9.  ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.normal()\n",
    "list(range(3,5))\n",
    "d = [1,2,40,4]\n",
    "w = [2, .5, 1]\n",
    "#np.dot(d[-3:],w)\n",
    "np.square(np.array(d[-3:]) - np.array(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter:\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = ProgressMeter._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def print2(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @classmethod\n",
    "    def _get_batch_fmtstr(cls, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "#LOOKS LIKE ALEXNET! besides linear(256*6*6=9216,4096) in alexnet for classifier\n",
    "class Net(nn.Module): #nn.module is being inherited here\n",
    "    def __init__(self, num_classes=10): #network architecture\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential( #feature extractor\n",
    "            #3,64 is input channel, output channel\n",
    "            #padding is zeros 2 in horizontal direction and verticle direction\n",
    "            #kernel can also be rectangle, specify by tuple, right now it's just 3x3\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2), \n",
    "            nn.ReLU(inplace=True), #inplace means it does processing at memory current memory spot, won't allocate new space for output\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        parameters_to_prune = ((model.features[0], \"weight\"),(model.features[3], \"weight\"),(model.features[6], \"weight\"),)\n",
    "        prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured,amount=10000,)\n",
    "        self.fc_layers = nn.Sequential( #classifier\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096), #fully connected layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        self.nclass = num_classes\n",
    "\n",
    "    def forward(self, x): # this is the main function that is called, for sending data forward through network layers\n",
    "        conv_features = self.features(x)\n",
    "        flatten = conv_features.view(conv_features.size(0), -1)\n",
    "        fc = self.fc_layers(flatten)\n",
    "        return fc\n",
    "\n",
    "\n",
    "def get_datasets(*args, **kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(train=True, transform=transform, *args, **kwargs)\n",
    "    testset = torchvision.datasets.CIFAR10(train=False, transform=transform, *args, **kwargs)\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "def get_dataloaders(trainset, testset, batch_size=100, num_worker=4):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_worker)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_worker)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def get_model(model_src_path, device='cpu'):\n",
    "    model = Net(num_classes=10)\n",
    "    state_dict = torch.load(model_src_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if type(output) is tuple:\n",
    "            _, _, output = output\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res, pred[0, :]\n",
    "\n",
    "\n",
    "def eval_single_batch_compute(x, y, model):\n",
    "    output = model(x)\n",
    "    accs, predictions = accuracy(output, y, topk=(1,))\n",
    "    acc = accs[0]\n",
    "    return acc, predictions\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, print_acc=False, device='cpu', log_update_feq=20):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(dataloader),\n",
    "        [top1],\n",
    "        prefix='Evaluating Batch'\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            x, y = data\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            n_data = y.size(0)\n",
    "\n",
    "            acc, predictions = eval_single_batch_compute(x, y, model)\n",
    "\n",
    "            top1.update(acc.item(), n_data)\n",
    "            if idx % log_update_feq == log_update_feq - 1:\n",
    "                progress.print2(idx + 1)\n",
    "\n",
    "        if print_acc:\n",
    "            print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Evaluating Batch[ 20/100]\tAcc@1  79.00 ( 79.85)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  77.00 ( 80.03)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  75.00 ( 80.03)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  85.00 ( 80.24)\n",
      "Evaluating Batch[100/100]\tAcc@1  82.00 ( 80.35)\n",
      " * Acc@1 80.350\n",
      "80.35\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using device:',device)\n",
    "trainset, testset = get_datasets(root='./data', download=True)\n",
    "_, testloader = get_dataloaders(trainset, testset, batch_size=100, num_worker=16)\n",
    "\n",
    "model_src_path = 'model.tar' # todo you need to set the path to downloaded model !!\n",
    "model = get_model(model_src_path, device)\n",
    "\n",
    "\n",
    "parameters_to_prune = ((model.features[0], \"weight\"),)\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured,amount=1000,)\n",
    "print(eval_model(model, testloader, print_acc=True, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Batch[ 20/100]\tAcc@1  86.00 ( 86.30)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  87.00 ( 86.33)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  87.00 ( 86.05)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  89.00 ( 86.14)\n",
      "Evaluating Batch[100/100]\tAcc@1  86.00 ( 86.24)\n",
      " * Acc@1 86.240\n",
      "Amt: 0 Accuracy 86.24\n",
      "Evaluating Batch[ 20/100]\tAcc@1  86.00 ( 86.30)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  87.00 ( 86.35)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  87.00 ( 86.07)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  88.00 ( 86.11)\n",
      "Evaluating Batch[100/100]\tAcc@1  85.00 ( 86.21)\n",
      " * Acc@1 86.210\n",
      "Amt: 100 Accuracy 86.21\n",
      "Evaluating Batch[ 20/100]\tAcc@1  86.00 ( 86.10)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  87.00 ( 86.30)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  87.00 ( 86.03)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  89.00 ( 86.10)\n",
      "Evaluating Batch[100/100]\tAcc@1  85.00 ( 86.20)\n",
      " * Acc@1 86.200\n",
      "Amt: 200 Accuracy 86.2\n",
      "Evaluating Batch[ 20/100]\tAcc@1  86.00 ( 86.10)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  87.00 ( 86.20)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  87.00 ( 85.92)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  88.00 ( 85.94)\n",
      "Evaluating Batch[100/100]\tAcc@1  86.00 ( 86.03)\n",
      " * Acc@1 86.030\n",
      "Amt: 300 Accuracy 86.03\n",
      "Evaluating Batch[ 20/100]\tAcc@1  86.00 ( 85.95)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  86.00 ( 86.15)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  87.00 ( 85.95)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  88.00 ( 86.05)\n",
      "Evaluating Batch[100/100]\tAcc@1  86.00 ( 86.18)\n",
      " * Acc@1 86.180\n",
      "Amt: 400 Accuracy 86.18\n",
      "Evaluating Batch[ 20/100]\tAcc@1  85.00 ( 85.20)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  86.00 ( 85.72)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  87.00 ( 85.75)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  89.00 ( 85.85)\n",
      "Evaluating Batch[100/100]\tAcc@1  89.00 ( 85.87)\n",
      " * Acc@1 85.870\n",
      "Amt: 500 Accuracy 85.87\n",
      "Evaluating Batch[ 20/100]\tAcc@1  85.00 ( 85.40)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  84.00 ( 85.53)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  86.00 ( 85.43)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  89.00 ( 85.58)\n",
      "Evaluating Batch[100/100]\tAcc@1  88.00 ( 85.71)\n",
      " * Acc@1 85.710\n",
      "Amt: 600 Accuracy 85.71\n",
      "Evaluating Batch[ 20/100]\tAcc@1  84.00 ( 84.75)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  83.00 ( 85.08)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  86.00 ( 85.07)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  89.00 ( 85.33)\n",
      "Evaluating Batch[100/100]\tAcc@1  87.00 ( 85.45)\n",
      " * Acc@1 85.450\n",
      "Amt: 700 Accuracy 85.45\n",
      "Evaluating Batch[ 20/100]\tAcc@1  82.00 ( 83.65)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  81.00 ( 83.22)\n",
      "Evaluating Batch[ 60/100]\tAcc@1  83.00 ( 83.15)\n",
      "Evaluating Batch[ 80/100]\tAcc@1  87.00 ( 83.49)\n",
      "Evaluating Batch[100/100]\tAcc@1  85.00 ( 83.48)\n",
      " * Acc@1 83.480\n",
      "Amt: 800 Accuracy 83.48\n",
      "Evaluating Batch[ 20/100]\tAcc@1  84.00 ( 82.90)\n",
      "Evaluating Batch[ 40/100]\tAcc@1  81.00 ( 82.75)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for amt in range(0,5001,100):\n",
    "    model_src_path = 'model.tar' # todo you need to set the path to downloaded model !!\n",
    "    model = get_model(model_src_path, device)\n",
    "    parameters_to_prune = ((model.features[0], \"weight\"),)\n",
    "    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured,amount=amt,)\n",
    "    ans= eval_model(model, testloader, print_acc=True, device=device)\n",
    "    print(\"Amt:\",amt, \"Accuracy\",ans)\n",
    "    results.append(ans)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Number of Pruned Weights\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(0,1000,5001), results)\n",
    "print(range(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)), 'weight'),\n",
       " (ReLU(inplace=True), 'weight'),\n",
       " (MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       "  'weight'),\n",
       " (Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n",
       "  'weight'),\n",
       " (ReLU(inplace=True), 'weight'),\n",
       " (MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       "  'weight'),\n",
       " (Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  'weight'),\n",
       " (ReLU(inplace=True), 'weight'),\n",
       " (Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  'weight'),\n",
       " (ReLU(inplace=True), 'weight'),\n",
       " (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  'weight'),\n",
       " (ReLU(inplace=True), 'weight'),\n",
       " (MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       "  'weight')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_to_prune = ((model.features[0], \"weight\"),(model.features[3], \"weight\"),(model.features[6], \"weight\"),)\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured,amount=10000,)\n",
    "\n",
    "eval_model(model, testloader, print_acc=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1000\n",
    "\n",
    "gn = [np.random.normal(),np.random.normal(),np.random.normal()]\n",
    "data = []\n",
    "for idx in range(3,n_sample+3):\n",
    "    g = np.random.normal()\n",
    "    gn.append(g)\n",
    "    data.append(.1*g+.5*gn[idx-1]-.5*gn[idx-2]+.1*gn[idx-3]) # x_[n] = u_[n] + 0.8*x_[n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emperical MSE estimate using 1000 samples:  0.5145843504024726\n"
     ]
    }
   ],
   "source": [
    "w = np.array([.4, -.1, .02])\n",
    "x = data[0:3]\n",
    "for idx in range(3,n_sample):\n",
    "    x.append(np.dot(x[-3:],w))\n",
    "print(\"Emperical MSE estimate using 1000 samples: \", np.sum(np.square(np.array(data) - np.array(x)))/n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "COV_X = []\n",
    "for three_sample_before, two_sample_before, one_sample_before, curr in zip(data[0:-3:], data[1:-2:], data[2:-1:],data[3::]):\n",
    "    vector1 = np.array([one_sample_before, two_sample_before, three_sample_before]) \n",
    "    vector2 = np.array([curr, one_sample_before, two_sample_before]) # for R\n",
    "    X.append(vector1)\n",
    "    y.append(curr)\n",
    "    COV_X.append(vector2)\n",
    "X = np.vstack(X)\n",
    "y = np.array(y)\n",
    "COV_X = np.vstack(COV_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weights: [-0.63856452 -0.3524288  -0.19029405]\n",
      "MSE: 0.3587265144264689\n"
     ]
    }
   ],
   "source": [
    "R = np.cov(COV_X.T)\n",
    "p = y.dot(X)/len(y)\n",
    "w = np.linalg.inv(R).dot(p)\n",
    "prediction = X.dot(w)\n",
    "print(\"Optimal Weights:\",w)\n",
    "error = y.dot(y)/n_sample - p.dot(w)\n",
    "print(\"MSE:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 50,\n",
       " 100,\n",
       " 150,\n",
       " 200,\n",
       " 250,\n",
       " 300,\n",
       " 350,\n",
       " 400,\n",
       " 450,\n",
       " 500,\n",
       " 550,\n",
       " 600,\n",
       " 650,\n",
       " 700,\n",
       " 750,\n",
       " 800,\n",
       " 850,\n",
       " 900,\n",
       " 950,\n",
       " 1000]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,1001,50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
