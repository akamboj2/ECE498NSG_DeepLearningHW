{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 64, 7, 2], [64, 64, 3, 1], [64, 64, 3, 1], [64, 64, 3, 1], [64, 64, 3, 1], [64, 128, 3, 2], [128, 128, 3, 1], [64, 128, 1, 2], [128, 128, 3, 1], [128, 128, 3, 1], [128, 256, 3, 2], [256, 256, 3, 1], [128, 256, 1, 2], [256, 256, 3, 1], [256, 256, 3, 1], [256, 512, 3, 2], [512, 512, 3, 1], [256, 512, 1, 2], [512, 512, 3, 1], [512, 512, 3, 1], [512, 1000]]\n",
      "[[3, 64, 3, 1], [64, 128, 3, 1], [128, 256, 3, 1], [256, 256, 3, 1], [256, 512, 3, 1], [512, 512, 3, 1], [512, 512, 3, 1], [512, 512, 3, 1], [25088, 4096], [4096, 4096], [4096, 1000]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTODO: \\n1) Do the same for above for VGG11! (Probs just copy paste and make a new array)\\n2) Calculate the Nl,Dl, Rla, Rlw\\n3) Finally do the compmutation in slide 7 here: https://courses.grainger.illinois.edu/ece498nsu/fa2020/secure/lectures/4-Finite-precision-DPs-2020-updated.pdf\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stuff to do: write code to scan this string, and find conv, df, in, out of 2d\n",
    "f = open(\"Resnet18.txt\")\n",
    "res_layers = []\n",
    "for line in f:\n",
    "    text = line.split()\n",
    "    if len(text)>1 and ((\"Conv\" in text[1])): # or (\"Linear\" in text[1])):\n",
    "        #print(line)\n",
    "        #in_chan, out_chan, k_size, stride\n",
    "        if \"Conv\" in text[1]:\n",
    "            in_chan = int(text[1].split('(')[1][:-1])\n",
    "            #print(in_chan)\n",
    "            out_chan = int(text[2][:-1])\n",
    "            #print(out_chan)\n",
    "            k_size = int(text[3].split('(')[1][:-1]) #assumes square kernel\n",
    "            #print(k_size)\n",
    "            stride =  int(text[5].split('(')[1][:-1])\n",
    "            #print(stride)\n",
    "        res_layers.append([in_chan,out_chan,k_size,stride])\n",
    "res_layers.append([512,1000])\n",
    "print(res_layers)\n",
    "\n",
    "f2 = open(\"VGG11.txt\")\n",
    "vgg_layers = []\n",
    "for line in f2:\n",
    "    text = line.split()\n",
    "    if len(text)>1 and ((\"Conv\" in text[1])): # or (\"Linear\" in text[1])):\n",
    "        #print(line)\n",
    "        #in_chan, out_chan, k_size, stride\n",
    "        if \"Conv\" in text[1]:\n",
    "            in_chan = int(text[1].split('(')[1][:-1])\n",
    "            #print(in_chan)\n",
    "            out_chan = int(text[2][:-1])\n",
    "            #print(out_chan)\n",
    "            k_size = int(text[3].split('(')[1][:-1]) #assumes square kernel\n",
    "            #print(k_size)\n",
    "            stride =  int(text[5].split('(')[1][:-1])\n",
    "            #print(stride)\n",
    "        vgg_layers.append([in_chan,out_chan,k_size,stride])\n",
    "vgg_layers.append([25088,4096])\n",
    "vgg_layers.append([4096,4096])\n",
    "vgg_layers.append([4096,1000])\n",
    "print(vgg_layers)\n",
    "\n",
    "\"\"\"\n",
    "TODO: \n",
    "1) Do the same for above for VGG11! (Probs just copy paste and make a new array)\n",
    "2) Calculate the Nl,Dl, Rla, Rlw\n",
    "3) Finally do the compmutation in slide 7 here: https://courses.grainger.illinois.edu/ece498nsu/fa2020/secure/lectures/4-Finite-precision-DPs-2020-updated.pdf\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet18:\n",
      "Computational Cost: 1,379,091,858,245.5996 \n",
      "Representational Cost: 184,419,200\n",
      "\n",
      "VGG11:\n",
      "Computational Cost: 1,251,552,292,824.7742 \n",
      "Representational Cost: 191,469,440\n"
     ]
    }
   ],
   "source": [
    "# for linear (numrow,numcols)\n",
    "# Nl Number of Dot Products is image_size^2*out_chan/stride in conv and len(col) aka numrows in linear\n",
    "# Dl dot product dim is kernalsize^2*in_chan and numcols in linear\n",
    "# Rla number of activations = out_chan*img_size^2 so numrows in linear\n",
    "# Rlw number of weightsRlw = k_size^2*in_chan*out_chan and numrows*numcols in linear\n",
    "\n",
    "#this assumes 32x32x3 input\n",
    "img_size = 32*32\n",
    "for NN in [res_layers,vgg_layers]:\n",
    "    CC, RC = 0, 0\n",
    "    for l in NN:\n",
    "        if len(l)==4: \n",
    "            #convolutional layer\n",
    "            in_chan, out_chan, k_size, stride = l\n",
    "            Nl = img_size * out_chan / stride\n",
    "            Dl = k_size**2 * in_chan\n",
    "            Rla = out_chan * img_size\n",
    "            Rlw = k_size**2 * in_chan*out_chan\n",
    "        else:\n",
    "            #linear layer\n",
    "            numrows, numcols = l\n",
    "            Nl = numrows\n",
    "            Dl = numcols\n",
    "            Rla = numrows\n",
    "            RLw = numrows*numcols\n",
    "        CC += Nl * (Dl * 10 * 10 + (Dl - 1) * (10 + 10 + np.log2(Dl) - 1))\n",
    "        RC += Rla * 10 + Rlw * 10\n",
    "    if NN == res_layers: \n",
    "        print(\"Resnet18:\")\n",
    "    else:\n",
    "        print(\"\\nVGG11:\")\n",
    "    print(\"Computational Cost:\",\"{:,}\".format(CC) ,\"\\nRepresentational Cost:\",\"{:,}\".format(RC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b784db79df01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class GradCollect:\n",
    "    inputs_grad_collect = []\n",
    "    weight_grad_collect = []\n",
    "    bias_grad_collect = []\n",
    "\n",
    "    weight_range_collect = []\n",
    "    bias_range_collect = []\n",
    "    activation_range_collect = []\n",
    "    activation_range_collect_temp = []\n",
    "\n",
    "    NUM_COMPUTE_LAYERS = 0\n",
    "    @classmethod\n",
    "    def retain_model_grad(cls, model):\n",
    "        def retain_module_grad(nn_module):\n",
    "            if isinstance(nn_module, nn.Conv2d) or isinstance(nn_module, nn.Linear):\n",
    "                print('registering weight parameters of {} layer'.format(nn_module._get_name()))\n",
    "                nn_module.weight.requires_grad_(True)\n",
    "                weight_stats = np.power(2.0, np.ceil(np.log2(np.amax(np.absolute(nn_module.weight.data.cpu().numpy())))))\n",
    "                GradCollect.weight_grad_collect.append((nn_module._get_name(), nn_module.weight))\n",
    "                GradCollect.weight_range_collect.append((nn_module._get_name(), weight_stats))\n",
    "\n",
    "        model.apply(retain_module_grad)\n",
    "        GradCollect.NUM_COMPUTE_LAYERS = len(GradCollect.weight_grad_collect)\n",
    "        GradCollect.activation_range_collect = np.zeros(GradCollect.NUM_COMPUTE_LAYERS)\n",
    "\n",
    "    @classmethod\n",
    "    def retain_inputs_grad(cls, model):\n",
    "        def retain_nn_module_inputs(m):\n",
    "            def retain_inputs(m, x):\n",
    "                x = x[0]\n",
    "                x = x.requires_grad_(True)\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    stats = np.power(2.0,np.ceil(np.log2(np.amax(np.absolute(x.detach().cpu().numpy())))))\n",
    "                    GradCollect.inputs_grad_collect.append((m._get_name(), x))\n",
    "                    GradCollect.activation_range_collect_temp.append((m._get_name(), stats))\n",
    "                    x.retain_grad()\n",
    "\n",
    "            m.register_forward_pre_hook(retain_inputs)\n",
    "\n",
    "        model.apply(retain_nn_module_inputs)\n",
    "\n",
    "\n",
    "def get_noise_gains(model, data_loader, device):\n",
    "    weight_gains = [0] * GradCollect.NUM_COMPUTE_LAYERS\n",
    "    activation_gains = [0] * GradCollect.NUM_COMPUTE_LAYERS\n",
    "    data_size = len(data_loader)\n",
    "    first_time = True\n",
    "    for inputs, targets in tqdm(data_loader):\n",
    "        inputs = inputs.to(device).requires_grad_(True)\n",
    "        GradCollect.inputs_grad_collect = []\n",
    "        GradCollect.activation_range_collect_temp = []\n",
    "        inputs.retain_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.sum(0)\n",
    "\n",
    "        GradCollect.activation_range_collect = np.vstack([\n",
    "            np.array([num for _, num in GradCollect.activation_range_collect_temp]),\n",
    "            GradCollect.activation_range_collect]\n",
    "        ).max(0)\n",
    "\n",
    "        Z_fl, Y_fl = outputs.max(0)\n",
    "        num_lbls = outputs.size(0)\n",
    "        for i in range(num_lbls):\n",
    "            if i != Y_fl:\n",
    "                output_difference = Z_fl - outputs[i]\n",
    "                output_difference.backward(retain_graph=True)\n",
    "                with torch.no_grad():\n",
    "                    denominator = 24 * (output_difference ** 2)\n",
    "                    for idx in range(GradCollect.NUM_COMPUTE_LAYERS):\n",
    "                        weight = GradCollect.weight_grad_collect[idx][1]\n",
    "                        weight_grad = weight.grad\n",
    "                        if first_time:\n",
    "                            weight_gains[idx] = (weight_grad ** 2).sum() / denominator\n",
    "                        else:\n",
    "                            weight_gains[idx].add_((weight_grad ** 2).sum() / denominator)\n",
    "\n",
    "                        weight.grad.zero_()\n",
    "\n",
    "                    for idx, (module_name, activations) in enumerate(GradCollect.inputs_grad_collect):\n",
    "                        grad = activations.grad\n",
    "                        if first_time:\n",
    "                            activation_gains[idx] = (grad ** 2).sum() / denominator\n",
    "                        else:\n",
    "                            activation_gains[idx].add_((grad ** 2).sum() / denominator)\n",
    "\n",
    "                        activations.grad.zero_()\n",
    "\n",
    "                    first_time = False\n",
    "\n",
    "    for idx in range(GradCollect.NUM_COMPUTE_LAYERS):\n",
    "        activation_gains[idx] = activation_gains[idx].cpu().numpy() / data_size\n",
    "        weight_gains[idx] = weight_gains[idx].cpu().numpy() / data_size\n",
    "\n",
    "    return weight_gains, activation_gains\n",
    "\n",
    "\n",
    "def get_normalized_noise_gains(wg_coarse, ag_coarse):\n",
    "    adjusted_wg_noise_gains = np.zeros(GradCollect.NUM_COMPUTE_LAYERS)\n",
    "    adjusted_ag_noise_gains = np.zeros(GradCollect.NUM_COMPUTE_LAYERS)\n",
    "\n",
    "    for l in range(GradCollect.NUM_COMPUTE_LAYERS):\n",
    "        adjusted_wg_noise_gains[l] = wg_coarse[l] * np.square(GradCollect.weight_range_collect[l][1])\n",
    "        adjusted_ag_noise_gains[l] = ag_coarse[l] * np.square(GradCollect.activation_range_collect[l])\n",
    "\n",
    "    min_ag = adjusted_ag_noise_gains.min()\n",
    "    min_wg = adjusted_wg_noise_gains.min()\n",
    "\n",
    "    least_gain = min(min_ag,min_wg)\n",
    "    return adjusted_wg_noise_gains, adjusted_ag_noise_gains, least_gain\n",
    "\n",
    "\n",
    "def get_precision_offsets(wg, ag, least_gain):\n",
    "    w_offsets = np.zeros(GradCollect.NUM_COMPUTE_LAYERS)\n",
    "    a_offsets = np.zeros(GradCollect.NUM_COMPUTE_LAYERS)\n",
    "\n",
    "    for l in range(GradCollect.NUM_COMPUTE_LAYERS):\n",
    "        w_offsets[l] = np.round(0.5 * np.log2(wg[l] / least_gain))\n",
    "        a_offsets[l] = np.round(0.5 * np.log2(ag[l] / least_gain))\n",
    "\n",
    "    return w_offsets, a_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-f59642586197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-f59642586197>\u001b[0m in \u001b[0;36mfeedforward\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[0mmodel_src_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'model.tar'\u001b[0m  \u001b[1;31m# todo you need to set the path to downloaded model !!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_src_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-f59642586197>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(model_src_path, device)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_src_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_src_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhi kamboj\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.tar'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "\n",
    "#from nn_inference import precision_profiler\n",
    "import numpy as np\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter:\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = ProgressMeter._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def print2(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @classmethod\n",
    "    def _get_batch_fmtstr(cls, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        self.nclass = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_features = self.features(x)\n",
    "        flatten = conv_features.view(conv_features.size(0), -1)\n",
    "        fc = self.fc_layers(flatten)\n",
    "        return fc\n",
    "\n",
    "\n",
    "def get_datasets(*args, **kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(train=True, transform=transform, *args, **kwargs)\n",
    "    testset = torchvision.datasets.CIFAR10(train=False, transform=transform, *args, **kwargs)\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "def get_dataloaders(trainset, testset, batch_size=100, num_worker=4):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_worker)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_worker)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def get_model(model_src_path, device='cpu'):\n",
    "    model = Net(num_classes=10)\n",
    "    state_dict = torch.load(model_src_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if type(output) is tuple:\n",
    "            _, _, output = output\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res, pred[0, :]\n",
    "\n",
    "\n",
    "def eval_single_batch_compute(x, y, model):\n",
    "    output = model(x)\n",
    "    accs, predictions = accuracy(output, y, topk=(1,))\n",
    "    acc = accs[0]\n",
    "    return acc, predictions\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, print_acc=False, device='cpu', log_update_feq=20):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(dataloader),\n",
    "        [top1],\n",
    "        prefix='Evaluating Batch'\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            x, y = data\n",
    "            x.requires_grad = True\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            n_data = y.size(0)\n",
    "\n",
    "            acc, predictions = eval_single_batch_compute(x, y, model)\n",
    "\n",
    "            top1.update(acc.item(), n_data)\n",
    "            if idx % log_update_feq == log_update_feq - 1:\n",
    "                progress.print2(idx + 1)\n",
    "\n",
    "        if print_acc:\n",
    "            print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def feedforward():\n",
    "    device = 'cpu'\n",
    "    print('using device:', device)\n",
    "    trainset, testset = get_datasets(root='./data', download=True)\n",
    "    _, testloader = get_dataloaders(trainset, testset, batch_size=100, num_worker=16)\n",
    "\n",
    "    model_src_path = 'model.tar'  # todo you need to set the path to downloaded model !!\n",
    "    model = get_model(model_src_path, device)\n",
    "    model = model.to(device)\n",
    "    eval_model(model, testloader, print_acc=True, device=device)\n",
    "\n",
    "\n",
    "def compute_precision_offsets():\n",
    "    device = 'cuda'\n",
    "    print('using device:', device)\n",
    "    trainset, testset = get_datasets(root='./data', download=True)\n",
    "    trainloader, testloader = get_dataloaders(trainset, testset, batch_size=500, num_worker=32)\n",
    "\n",
    "    model_src_path = 'model.tar'\n",
    "    model = get_model(model_src_path, device)\n",
    "    model = model.to(device)\n",
    "    precision_profiler.GradCollect.retain_model_grad(model)\n",
    "    precision_profiler.GradCollect.retain_inputs_grad(model)\n",
    "\n",
    "    wg, ag = precision_profiler.get_noise_gains(model, trainloader, device)\n",
    "    wg, ag, least_gain = precision_profiler.get_normalized_noise_gains(wg, ag)\n",
    "    print(precision_profiler.GradCollect.weight_range_collect)\n",
    "    print(precision_profiler.GradCollect.activation_range_collect)\n",
    "    w_offsets, a_offsets = precision_profiler.get_precision_offsets(wg, ag, least_gain)\n",
    "    print(w_offsets, a_offsets)\n",
    "    np.save(arr=w_offsets,file='weight_offsets.npy')\n",
    "    np.save(arr=a_offsets,file='activation_offsets.npy')\n",
    "    np.save(arr=precision_profiler.GradCollect.activation_range_collect, file='activation_dynamic_range.npy')\n",
    "    np.save(arr=np.array([v for k, v in precision_profiler.GradCollect.weight_range_collect]), file='weight_dynamic_range.npy')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    feedforward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. 16. 16. 16.  4.  4.  4.  8.]\n",
      "[1.      0.5     0.5     0.25    0.25    0.03125 0.03125 0.5    ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4) Hints\\nPlot the convergence curve means plot MSE convergence curve, and plot weights!\\nconvergence around 3.1, conversion around -.7 or -.8\\nyou will see many different values\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.3 --> Write down gradient and update equation!\n",
    "#3.1 you need dynamic ranges, then do quantization\n",
    "#3.2 you need all four files\n",
    "activation_dr= np.load(\"activation_dynamic_range.npy\") \n",
    "#each index of the array corresponds to each layer and that will have maximum value given by\n",
    "weight_dr=np.load(\"weight_dynamic_range.npy\")\n",
    "print(activation_dr)\n",
    "print(weight_dr)\n",
    "\n",
    "# 4 6 7, for any of the weights, put all weights in plot and give the numbers\n",
    "\"\"\"4) Hints:\n",
    "Plot the convergence curve means plot MSE convergence curve, and plot weights!\n",
    "convergence around 3.1, conversion around -.7 or -.8\n",
    "you will see many different values\n",
    "pay attention to initialization! That will change result a lot.\n",
    "\n",
    "if you use a lot of samples, it should match your analytic evaluation\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p7 critical path delay, most amount of time you need to get to end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2  0.7  0.2 -0.3 -0.8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def UniformLevels(r,m):\n",
    "    #this will place levels at either end of r and uniformly between\n",
    "    return np.arange(-r,r+r/m,2*r/(m-1))\n",
    "    \n",
    "print(np.array([.2])-UniformLevels(1,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
